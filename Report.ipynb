{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbeb43cd",
   "metadata": {},
   "source": [
    "<img src=\"https://www.dept.aueb.gr/sites/default/files/BA.jpg\" title=\"PROGRAME Logo\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f5e61a",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkblue\"> <hr>Athens University of Economics and Business<br> M.Sc. Business Analytics (Part Time)<br>Mining Big Datasets, Kotidis Ioannis / Ioanna Fillipidou <hr><br>Athanasios Alexandris p2822202<br> Ioannis Demertzis p2822210<br> 28 – 05 – 23<hr>\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a20b5b7",
   "metadata": {},
   "source": [
    "## `Assignment I`\n",
    "This notebook contains the task descriptions for the first assignment, as well as the fully commented code produced to deliver these tasks along with comments and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d20001",
   "metadata": {},
   "source": [
    "### <hr>`Task 1- Import and pre-process the dataset with users`\n",
    "\n",
    "Download the movieLens dataset from moodle. This dataset includes **100.000** ratings (1-5) from **943** users on **1682** movies. Each user has rated **at least 20 movies**. There are **3 files** for the dataset:  \n",
    "- the **users.txt** file contains id, age, gender, occupation, and postcode separated by | \n",
    "- the **movies.txt** file contains id, title (with release year) and some other information not related with the assignment separated by | \n",
    "- the **ratings.txt** file (tab separated) which contains userid, movieid, rating (1-5) and timestamp. \n",
    "\n",
    "For this assignment you will only use the set of movies that **a user has rated and not the ratings**. \n",
    "In your report you should **describe in detail** any processing and conversion you made to the original data and the **reasons** it was necessary.<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c916b1a7",
   "metadata": {},
   "source": [
    "#### Handling the Import Error\n",
    "While trying to import the files an error came up regarding the file's encoding. \\\n",
    "To tackle this, we first tried to identify the encoding of each file, as seen in the following code chunk, and then decided which encoding was the correct one for reading the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d5751fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users.txt: ascii\n",
      "movies.txt: ISO-8859-1\n",
      "ratings.txt: ascii\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import chardet\n",
    "\n",
    "# List of file names\n",
    "files = ['users.txt', 'movies.txt', 'ratings.txt']\n",
    "\n",
    "# Iterate over file names\n",
    "for filename in files:\n",
    "    \n",
    "    # Detect the encoding of a text file\n",
    "    \n",
    "    # \"With\" statement to ensure that the file is properly closed after it is read.\n",
    "    # \"Open\" function to open the file in binary mode \"rb\" and store to the variable \"f\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        \n",
    "        # \"Read\" function and \"detect\" function to identify the encoding\n",
    "        result = chardet.detect(f.read()) \n",
    "    \n",
    "    # Print the detected encoding\n",
    "    print(f'{filename}: {result[\"encoding\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97d409c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data files by specifying the correct encoding\n",
    "users = pd.read_csv('users.txt', \n",
    "                    sep='|', \n",
    "                    names=['userid', 'age', 'gender', 'occupation', 'postcode'], \n",
    "                    encoding='ascii')\n",
    "\n",
    "movies = pd.read_csv('movies.txt',\n",
    "                     usecols=[0, 1],\n",
    "                     sep='|', \n",
    "                     names=['movieid', 'title'], \n",
    "                     encoding='ISO-8859-1')\n",
    "\n",
    "ratings = pd.read_csv('ratings.txt', \n",
    "                      sep='\\t', \n",
    "                      names=['userid', 'movieid', 'rating', 'timestamp'], \n",
    "                      encoding='ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd48d37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframes\n",
    "data = pd.merge(pd.merge(users, ratings, on='userid'), movies, on='movieid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0110794b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "data = data.drop(['rating', 'timestamp'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2858f111",
   "metadata": {},
   "source": [
    "#### Explaining the Steps\n",
    "\n",
    "1- To combine the three datataframes into one complete dataset, we used the **`merge()`** function. An inner join was performed to only include rows where there was a match between the **userid** and **movieid** columns in the users, movies, and ratings dataframes. This means that the resulting dataframe contained information about users who had rated movies and movies that had been rated by users.\n",
    "\n",
    "2- The **rating** and **timestamp** columns were dropped as it was requested to form a dataset using only the set of movies that a user has rated and not the ratings themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8062d4fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>occupation</th>\n",
       "      <th>postcode</th>\n",
       "      <th>movieid</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>M</td>\n",
       "      <td>technician</td>\n",
       "      <td>85711</td>\n",
       "      <td>61</td>\n",
       "      <td>Three Colors: White (1994)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>47</td>\n",
       "      <td>M</td>\n",
       "      <td>educator</td>\n",
       "      <td>29206</td>\n",
       "      <td>61</td>\n",
       "      <td>Three Colors: White (1994)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18</td>\n",
       "      <td>35</td>\n",
       "      <td>F</td>\n",
       "      <td>other</td>\n",
       "      <td>37212</td>\n",
       "      <td>61</td>\n",
       "      <td>Three Colors: White (1994)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58</td>\n",
       "      <td>27</td>\n",
       "      <td>M</td>\n",
       "      <td>programmer</td>\n",
       "      <td>52246</td>\n",
       "      <td>61</td>\n",
       "      <td>Three Colors: White (1994)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59</td>\n",
       "      <td>49</td>\n",
       "      <td>M</td>\n",
       "      <td>educator</td>\n",
       "      <td>08403</td>\n",
       "      <td>61</td>\n",
       "      <td>Three Colors: White (1994)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>863</td>\n",
       "      <td>17</td>\n",
       "      <td>M</td>\n",
       "      <td>student</td>\n",
       "      <td>60089</td>\n",
       "      <td>1679</td>\n",
       "      <td>B. Monkey (1998)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>863</td>\n",
       "      <td>17</td>\n",
       "      <td>M</td>\n",
       "      <td>student</td>\n",
       "      <td>60089</td>\n",
       "      <td>1678</td>\n",
       "      <td>Mat' i syn (1997)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>863</td>\n",
       "      <td>17</td>\n",
       "      <td>M</td>\n",
       "      <td>student</td>\n",
       "      <td>60089</td>\n",
       "      <td>1680</td>\n",
       "      <td>Sliding Doors (1998)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>896</td>\n",
       "      <td>28</td>\n",
       "      <td>M</td>\n",
       "      <td>writer</td>\n",
       "      <td>91505</td>\n",
       "      <td>1681</td>\n",
       "      <td>You So Crazy (1994)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>916</td>\n",
       "      <td>27</td>\n",
       "      <td>M</td>\n",
       "      <td>engineer</td>\n",
       "      <td>N2L5N</td>\n",
       "      <td>1682</td>\n",
       "      <td>Scream of Stone (Schrei aus Stein) (1991)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       userid  age gender  occupation postcode  movieid  \\\n",
       "0           1   24      M  technician    85711       61   \n",
       "1          13   47      M    educator    29206       61   \n",
       "2          18   35      F       other    37212       61   \n",
       "3          58   27      M  programmer    52246       61   \n",
       "4          59   49      M    educator    08403       61   \n",
       "...       ...  ...    ...         ...      ...      ...   \n",
       "99995     863   17      M     student    60089     1679   \n",
       "99996     863   17      M     student    60089     1678   \n",
       "99997     863   17      M     student    60089     1680   \n",
       "99998     896   28      M      writer    91505     1681   \n",
       "99999     916   27      M    engineer    N2L5N     1682   \n",
       "\n",
       "                                           title  \n",
       "0                     Three Colors: White (1994)  \n",
       "1                     Three Colors: White (1994)  \n",
       "2                     Three Colors: White (1994)  \n",
       "3                     Three Colors: White (1994)  \n",
       "4                     Three Colors: White (1994)  \n",
       "...                                          ...  \n",
       "99995                           B. Monkey (1998)  \n",
       "99996                          Mat' i syn (1997)  \n",
       "99997                       Sliding Doors (1998)  \n",
       "99998                        You So Crazy (1994)  \n",
       "99999  Scream of Stone (Schrei aus Stein) (1991)  \n",
       "\n",
       "[100000 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display resulting dataframe\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63358ddc",
   "metadata": {},
   "source": [
    "#### <hr>Data Cleaning<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0b1d72",
   "metadata": {},
   "source": [
    "For data cleaning purposes, we checked for missing values in the data dataframe by using the **`isnull()`** function. No missing values in any of the variables were found.\n",
    "\n",
    "Also, in order to gain a better understanding of the data and identify any \"extreme\" or \"strange\" values, we used the **`describe()`** function on the data dataframe. This provided us with descriptive statistics, including count, mean, standard deviation, minimum, quartiles, and maximum values for each numerical column in the dataframe.\n",
    "\n",
    "Nothing unusual was detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7315102c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userid        0\n",
       "age           0\n",
       "gender        0\n",
       "occupation    0\n",
       "postcode      0\n",
       "movieid       0\n",
       "title         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d2258bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>age</th>\n",
       "      <th>movieid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.00000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>462.48475</td>\n",
       "      <td>32.969850</td>\n",
       "      <td>425.530130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>266.61442</td>\n",
       "      <td>11.562623</td>\n",
       "      <td>330.798356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>254.00000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>175.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>447.00000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>322.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>682.00000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>631.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>943.00000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>1682.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             userid            age        movieid\n",
       "count  100000.00000  100000.000000  100000.000000\n",
       "mean      462.48475      32.969850     425.530130\n",
       "std       266.61442      11.562623     330.798356\n",
       "min         1.00000       7.000000       1.000000\n",
       "25%       254.00000      24.000000     175.000000\n",
       "50%       447.00000      30.000000     322.000000\n",
       "75%       682.00000      40.000000     631.000000\n",
       "max       943.00000      73.000000    1682.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Understand if there are \"extreme\" or \"strange\" values (eg. age 1000)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30eb2c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>postcode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>V0R2M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>V1G4L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792</th>\n",
       "      <td>V3N4P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793</th>\n",
       "      <td>V5A2B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>Y1A6B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>795 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    postcode\n",
       "0      00000\n",
       "1      01002\n",
       "2      01040\n",
       "3      01080\n",
       "4      01331\n",
       "..       ...\n",
       "790    V0R2M\n",
       "791    V1G4L\n",
       "792    V3N4P\n",
       "793    V5A2B\n",
       "794    Y1A6B\n",
       "\n",
       "[795 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get unique values for \"postcode\"\n",
    "unique_postcodes= data['postcode'].unique()\n",
    "\n",
    "# Sort unique values in lexicographic order\n",
    "unique_postcodes.sort()\n",
    "\n",
    "# Display the head and tail of the unique post codes\n",
    "postcodes = pd.DataFrame(unique_postcodes, columns=['postcode'])\n",
    "postcodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d50cf6",
   "metadata": {},
   "source": [
    "Based on the above it seemed tha most postal codes contained 5 numbers but there were also 17 postcodes which were a mix of letters and numbers. \n",
    "\n",
    "After a quick search online, these 2 different post code types corresponded to USA and Canada postcodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ef4a34c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['administrator' 'artist' 'doctor' 'educator' 'engineer' 'entertainment'\n",
      " 'executive' 'healthcare' 'homemaker' 'lawyer' 'librarian' 'marketing'\n",
      " 'none' 'other' 'programmer' 'retired' 'salesman' 'scientist' 'student'\n",
      " 'technician' 'writer']\n"
     ]
    }
   ],
   "source": [
    "# Get unique values for \"οccupation\"\n",
    "unique_occupation= data['occupation'].unique()\n",
    "\n",
    "# Sort unique values in lexicographic order\n",
    "unique_occupation.sort()\n",
    "\n",
    "# Print sorted unique values\n",
    "print(unique_occupation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a681709",
   "metadata": {},
   "source": [
    "### <hr>`Task 2 - Compute exact Jaccard similarity of users`\n",
    "\n",
    "To assess the similarity between users you should compute the **exact Jaccard Similarity for all pairs of users** and only output the pairs of users (unique) that have similarity **at least 0,5 (>=50%)**. For each pair denote their ids and the similarity score.<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ee9cc9",
   "metadata": {},
   "source": [
    "#### The Process of Calculating the Similarity\n",
    "The Jaccard Similarity between two users is defined as the size of the intersection of the sets of movies they have rated divided by the size of the union of the sets of movies they have rated.\n",
    "\n",
    "Brute force calculating the Jaccard similarity can be computationally expensive for large datasets because it computes it for all pairs of users, which has a time complexity of `O(n^2)` where `n` is the number of users.\n",
    "\n",
    "For that reason a dictionary was used to store the sets of movies rated by each user. The dictionary was populated by iterating over the unique user ids and indexing the dataframe using boolean indexing to get the set of movies rated by each user and then calculating the Jaccard similarity between them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5eb785",
   "metadata": {},
   "source": [
    "**Process explanation**: \n",
    "- First, we obtained the unique user IDs from the dataset. \n",
    "- Then, we initialized a dictionary called **movies_user** to store sets of movies rated by each user. To populate this dictionary, we iterated over the unique user IDs. For each user ID, we selected the rows where the **userid** value matched the current user ID. From these selected rows, we extracted the **movieid** column and converted it into a set using the set function.\n",
    "- Finally, we assigned this set of movies to the corresponding key in the **movies_user** dictionary. This way, we obtained a dictionary that mapped each user ID to a set of movies they had rated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "036608ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique user ids\n",
    "userids = data['userid'].unique()\n",
    "\n",
    "# Initialize a dictionary to store sets of movies rated by each user\n",
    "movies_user = {}\n",
    "\n",
    "# Populate dictionary with sets of movies rated by each user\n",
    "\n",
    "# Iterate over the unique user ids and select only the rows where the \"userid\" value is equal to the current user id. \n",
    "for userid in userids:\n",
    "    \n",
    "    # The \"movieid\" column is then selected and converted into a set using the \"set\" function.\n",
    "    # This set is then assigned to the corresponding key in the movies dictionary\n",
    "    movies_user[userid] = set(data.loc[data['userid'] == userid, 'movieid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a516a94",
   "metadata": {},
   "source": [
    "To perform pairwise iterations between users, we imported the **`combinations`** module from the itertools library. After initializing an empty list called **results**, we iterated over all pairs of users using the **`combinations()`** function. For each pair of users, we accessed their respective sets of movies rated, **movies_1** and **movies_2**, from the **movies_user** dictionary.\n",
    "\n",
    "To compute the Jaccard similarity between the two sets, we calculated the intersection of movies rated by both users and the union of movies rated by either user. The Jaccard similarity was then determined by dividing the size of the intersection by the size of the union. \n",
    "If the Jaccard similarity was found to be at least 50%, we stored the result by appending a tuple containing the user IDs and the Jaccard similarity to the results list.\n",
    "\n",
    "By applying this process to all possible pairs of users, we obtained a list of results representing pairs of users with a Jaccard similarity of at least 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d904f190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 5.247977256774902 seconds\n"
     ]
    }
   ],
   "source": [
    "# Import required library to perform pairwise iterations\n",
    "from itertools import combinations\n",
    "\n",
    "# Calculate the start time of the code execution\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Iterate over all pairs of users\n",
    "for user1, user2 in combinations(userids, 2):\n",
    "    \n",
    "    # Get sets of movies rated by each user\n",
    "    \n",
    "    # Set of movies rated by the first user from the movies dictionary\n",
    "    movies_1 = movies_user[user1]\n",
    "    \n",
    "    # Set of movies rated by the second user from the movies dictionary\n",
    "    movies_2 = movies_user[user2]\n",
    "    \n",
    "    # Compute the Jaccard similarity\n",
    "    \n",
    "    # Intersection = only the movies that are present in both sets\n",
    "    intersection = movies_1 & movies_2\n",
    "    \n",
    "    # Union = only the movies that are present in either set\n",
    "    union = movies_1 | movies_2\n",
    "    \n",
    "    # Jaccard Similarity = the size of the intersection divided by the size of the union\n",
    "    jaccard_similarity = len(intersection) / len(union)\n",
    "    \n",
    "    # Check if Jaccard similarity is at least 50%\n",
    "    if jaccard_similarity >= 0.5:\n",
    "        \n",
    "        # Store result\n",
    "        results.append((user1, user2, jaccard_similarity))\n",
    "\n",
    "# Calculate the end time and elapsed time of the code execution\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# Print the elapsed time of the code execution\n",
    "print(f\"Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc23142",
   "metadata": {},
   "source": [
    "Next, we displayed the pair of users that have similarity greater than 0.5, sorted in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04f455e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user1</th>\n",
       "      <th>user2</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>408</td>\n",
       "      <td>898</td>\n",
       "      <td>0.838710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>328</td>\n",
       "      <td>788</td>\n",
       "      <td>0.672956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>489</td>\n",
       "      <td>587</td>\n",
       "      <td>0.629921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>826</td>\n",
       "      <td>600</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>451</td>\n",
       "      <td>489</td>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>674</td>\n",
       "      <td>879</td>\n",
       "      <td>0.521739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>554</td>\n",
       "      <td>764</td>\n",
       "      <td>0.517007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>197</td>\n",
       "      <td>826</td>\n",
       "      <td>0.512987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>197</td>\n",
       "      <td>600</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>879</td>\n",
       "      <td>800</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user1  user2  similarity\n",
       "9    408    898    0.838710\n",
       "4    328    788    0.672956\n",
       "8    489    587    0.629921\n",
       "2    826    600    0.545455\n",
       "7    451    489    0.533333\n",
       "5    674    879    0.521739\n",
       "3    554    764    0.517007\n",
       "0    197    826    0.512987\n",
       "1    197    600    0.500000\n",
       "6    879    800    0.500000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create result dataframe\n",
    "result_df = pd.DataFrame(results, columns=['user1', 'user2', 'similarity'])\n",
    "\n",
    "# Display result dataframe\n",
    "result_df.sort_values(by='similarity', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2beaae",
   "metadata": {},
   "source": [
    "Then, we found the pair with the maximum Jaccard similarity from the results list. The pair with the highest similarity was determined by using the **`max()`** function, with the key parameter set to a **`lambda`** function that accessed the Jaccard similarity value from index position 2. The user IDs of this pair were assigned to **user1** and **user2**.\n",
    "\n",
    "To get the sets of movies rated by each user, we accessed the respective user's set of movies from the **movies_user** dictionary. The set of movies rated by user1 was assigned to **movies_1**, and the set of movies rated by user2 was assigned to **movies_2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d625fad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the pair with maximum Jaccard similarity (index 2)\n",
    "max_pair = max(results, key=lambda x: x[2])\n",
    "user1, user2 = max_pair[0], max_pair[1]\n",
    "\n",
    "# Get sets of movies rated by each user   \n",
    "# Set of movies rated by the first user from the movies dictionary\n",
    "movies_1 = movies_user[user1]\n",
    "# Set of movies rated by the second user from the movies dictionary\n",
    "movies_2 = movies_user[user2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496b8558",
   "metadata": {},
   "source": [
    "We then obtained the movie names corresponding to the movie IDs in **movies_1** and **movies_2**. This was achieved by creating a dictionary (**movie_names**) that mapped movie IDs to movie names from the movies dataset. We used this dictionary to retrieve the movie names for each user by iterating over their respective sets of movies.\n",
    "\n",
    "Next, we calculated the intersection of the two sets (**movies_1** and **movies_2**) to identify the common movies rated by both users. We obtained the movie names of this intersection by referencing the **movie_names** dictionary.\n",
    "\n",
    "Similarly, we calculated the union of the two sets to determine the total movies seen by the two users combined. We retrieved the movie names of this union using the **movie_names** dictionary.\n",
    "\n",
    "Finally, we printed the results in a sorted manner. We displayed the movies seen by user1 along with the count of movies. Similarly, we displayed the movies seen by user2 and their count. We also printed the movies seen by both users and their count. Additionally, we displayed the total movies seen by both users and their count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c11ff84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the movies names per movie id\n",
    "movie_names = dict(zip(movies['movieid'], movies['title']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de7a6eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies Seen From User 1:\n",
      "['Air Force One (1997)', 'Apt Pupil (1998)', 'Conspiracy Theory (1997)', 'Contact (1997)', 'Cop Land (1997)', 'English Patient, The (1996)', 'Everyone Says I Love You (1996)', 'Gattaca (1997)', 'Good Will Hunting (1997)', 'Indian Summer (1996)', 'Jackal, The (1997)', 'Kolya (1996)', 'L.A. Confidential (1997)', 'Liar Liar (1997)', 'Lost Highway (1997)', 'Midnight in the Garden of Good and Evil (1997)', 'Mouse Hunt (1997)', 'Rainmaker, The (1997)', 'Rocket Man (1997)', 'Saint, The (1997)', 'Scream (1996)', 'Spawn (1997)', 'Starship Troopers (1997)', 'Titanic (1997)', 'Tomorrow Never Dies (1997)', 'U Turn (1997)', 'Wag the Dog (1997)']\n",
      "Number of Movies:\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# Get the movie names for user 1 from the pair with highest similarity\n",
    "user1_movies = [movie_names[movie_id] for movie_id in movies_1]\n",
    "print(f'Movies Seen From User 1:\\n{sorted(user1_movies)}')\n",
    "print(f'Number of Movies:\\n{len(user1_movies)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be4072a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies Seen From User 2:\n",
      "['Air Force One (1997)', 'Alien: Resurrection (1997)', 'Apt Pupil (1998)', 'As Good As It Gets (1997)', 'Conspiracy Theory (1997)', 'Contact (1997)', 'Cop Land (1997)', 'Deceiver (1997)', 'English Patient, The (1996)', 'Everyone Says I Love You (1996)', 'Gattaca (1997)', 'Good Will Hunting (1997)', 'Indian Summer (1996)', 'Jackal, The (1997)', 'Jungle2Jungle (1997)', 'Kolya (1996)', 'L.A. Confidential (1997)', 'Lost Highway (1997)', 'Midnight in the Garden of Good and Evil (1997)', 'Mouse Hunt (1997)', 'Rainmaker, The (1997)', 'Rocket Man (1997)', 'Saint, The (1997)', 'Scream (1996)', 'Spawn (1997)', 'Starship Troopers (1997)', 'Titanic (1997)', 'Tomorrow Never Dies (1997)', 'U Turn (1997)', 'Wag the Dog (1997)']\n",
      "Number of Movies:\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "# Get the movie names for user 2 from the pair with highest similarity\n",
    "user2_movies = [movie_names[movie_id] for movie_id in movies_2]\n",
    "print(f'Movies Seen From User 2:\\n{sorted(user2_movies)}')\n",
    "print(f'Number of Movies:\\n{len(user2_movies)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0a4e498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies Seen From Both Users:\n",
      "['Air Force One (1997)', 'Apt Pupil (1998)', 'Conspiracy Theory (1997)', 'Contact (1997)', 'Cop Land (1997)', 'English Patient, The (1996)', 'Everyone Says I Love You (1996)', 'Gattaca (1997)', 'Good Will Hunting (1997)', 'Indian Summer (1996)', 'Jackal, The (1997)', 'Kolya (1996)', 'L.A. Confidential (1997)', 'Lost Highway (1997)', 'Midnight in the Garden of Good and Evil (1997)', 'Mouse Hunt (1997)', 'Rainmaker, The (1997)', 'Rocket Man (1997)', 'Saint, The (1997)', 'Scream (1996)', 'Spawn (1997)', 'Starship Troopers (1997)', 'Titanic (1997)', 'Tomorrow Never Dies (1997)', 'U Turn (1997)', 'Wag the Dog (1997)']\n",
      "Number of Common Movies:\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "# Compute the intersection of the two sets\n",
    "intersection = movies_1 & movies_2\n",
    "\n",
    "# Get movie names of the intersection\n",
    "intersection_movie_names = [movie_names[movie_id] for movie_id in intersection]\n",
    "\n",
    "# Print output sorted in ascending order\n",
    "print(f'Movies Seen From Both Users:\\n{sorted(intersection_movie_names)}')\n",
    "print(f'Number of Common Movies:\\n{len(intersection_movie_names)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ed032a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Movies Seen From The Users:\n",
      "['Air Force One (1997)', 'Alien: Resurrection (1997)', 'Apt Pupil (1998)', 'As Good As It Gets (1997)', 'Conspiracy Theory (1997)', 'Contact (1997)', 'Cop Land (1997)', 'Deceiver (1997)', 'English Patient, The (1996)', 'Everyone Says I Love You (1996)', 'Gattaca (1997)', 'Good Will Hunting (1997)', 'Indian Summer (1996)', 'Jackal, The (1997)', 'Jungle2Jungle (1997)', 'Kolya (1996)', 'L.A. Confidential (1997)', 'Liar Liar (1997)', 'Lost Highway (1997)', 'Midnight in the Garden of Good and Evil (1997)', 'Mouse Hunt (1997)', 'Rainmaker, The (1997)', 'Rocket Man (1997)', 'Saint, The (1997)', 'Scream (1996)', 'Spawn (1997)', 'Starship Troopers (1997)', 'Titanic (1997)', 'Tomorrow Never Dies (1997)', 'U Turn (1997)', 'Wag the Dog (1997)']\n",
      "Number of Total User Movies:\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "# Compute the union of the two sets\n",
    "union = movies_1 | movies_2\n",
    "\n",
    "# Get movie names of the union\n",
    "union_movie_names = [movie_names[movie_id] for movie_id in union]\n",
    "\n",
    "# Print output sorted in ascending order\n",
    "print(f'Total Movies Seen From The Users:\\n{sorted(union_movie_names)}')\n",
    "print(f'Number of Total User Movies:\\n{len(union_movie_names)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964b1e7c",
   "metadata": {},
   "source": [
    "### <hr>`Task 3 - Compute similarity using Min-hash signatures`\n",
    "\n",
    "In this step you compute min-hash signatures for each user and use them to evaluate their similarity.\n",
    "\n",
    "**Description of hash functions**: use the following family of hash functions: `ha,b(x)=(ax+b) mod R`, with a,b random integers in the interval (0,R) and R a large enough prime number that you may want to finetune in your initial experimentation. Make sure that each hash function uses different values of a,b pairs.\n",
    "\n",
    "**Evaluation of Min-hashing**: Use 50, 100, and 200 hash functions. For each value, output the pair of users that have estimated similarity at least 0.5, and report the number of false positives and false negatives (against the exact Jaccard similarity) that you obtain. For the false positives and negatives, report the averages for 5 different runs using different functions. Comment on how the number of hash functions affects the false positive and negatives figures.<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3066a063",
   "metadata": {},
   "source": [
    "#### The Process of Computing Similarity Using Min-hash Signatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a995f057",
   "metadata": {},
   "source": [
    "To begin with, we defined the following two following functions: \n",
    "\n",
    "- **`get_similarity()`**: \n",
    "This function calculates the similarity between two signatures. It first counts the number of elements that are equal in both signatures by using a generator expression and the **`zip()`** function to compare corresponding elements of the two signatures. Then, it calculates the **similarity** as the ratio of the number of matching elements to the total number of elements in the signatures.\n",
    "\n",
    "- **`jaccard_sim()`**: \n",
    "This function calculates the Jaccard similarity between two lists. It first calculates and returns a set containing the common elements of the two lists using the **intersection** method. Next, it calculates the **union** of the two lists as the sum of their lengths minus the length of their intersection. Finally, it calculates the Jaccard similarity as the ratio of the intersection size to the union size and returns it as a float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5173334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity(signature1, signature2):\n",
    "    # Count the number of elements that are equal in both signatures\n",
    "    num_matches = sum(a == b for a, b in zip(signature1, signature2))\n",
    "    \n",
    "    # Calculate the similarity as the ratio of matching elements to the total number of elements\n",
    "    similarity = num_matches / len(signature1)\n",
    "    \n",
    "    # Return the calculated similarity\n",
    "    return similarity\n",
    "\n",
    "def jaccard_sim(list1, list2):\n",
    "    # Calculate the intersection of the two lists by converting them to sets and using the set intersection method\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    \n",
    "    # Calculate the union of the two lists as the sum of their lengths minus the length of their intersection\n",
    "    union = (len(list1) + len(list2)) - intersection\n",
    "    \n",
    "    # Calculate the Jaccard similarity as the ratio of the intersection to the union\n",
    "    return float(intersection) / union"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be704b2",
   "metadata": {},
   "source": [
    "To compute similarity using Min-hash signatures we used the code as seen below. \n",
    "\n",
    "In summary, the code performs multiple runs with different random hash functions, generates MinHash signatures for each user, calculates similarities between users, and evaluates false positives and false negatives. The average results are then calculated and printed for each run and for different numbers of hash functions tested. \n",
    "\n",
    "In more detail: \n",
    "- First we started by setting the maximum value for the hash functions, which in this case is R = 10009.\n",
    "- Then we proceeded to define a list of numbers of hash functions to test, which included values of 50, 100, and 200.\n",
    "- Additionally, we specified the similarity threshold as 0.5 and the number of runs as 5. \n",
    "- We continued by iterating over the list of hash functions, performing the following steps:\n",
    "    - We created lists and variables to store the results for each run, including the lengths of similar users, the average number of similar users, and the average false positives and false negatives.\n",
    "    - Within each run, we generated different random hash functions by selecting random values for 'a' and 'b'. These hash functions were used to calculate MinHash signatures for each user based on their associated movies. \n",
    "    - The similarities between pairs of users were then calculated by comparing their MinHash signatures. If the similarity was above the threshold, the pair of users was considered similar and added to the list of similar users. \n",
    "    - False positives and false negatives were also evaluated by comparing the MinHash similarity to the exact Jaccard similarity. \n",
    "\n",
    "The process was repeated for the specified number of runs, and the average number of similar users, false positives, and false negatives were calculated over all runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50b7bd60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 Hash functions Results\n",
      "-------------------------------------------\n",
      "Number of similar users: 90\n",
      "Number of false positives: 81\n",
      "Number of false negatives: 1\n",
      "-------------------------------------------\n",
      "Number of similar users: 88\n",
      "Number of false positives: 80\n",
      "Number of false negatives: 2\n",
      "-------------------------------------------\n",
      "Number of similar users: 75\n",
      "Number of false positives: 66\n",
      "Number of false negatives: 1\n",
      "-------------------------------------------\n",
      "Number of similar users: 124\n",
      "Number of false positives: 116\n",
      "Number of false negatives: 2\n",
      "-------------------------------------------\n",
      "Number of similar users: 59\n",
      "Number of false positives: 53\n",
      "Number of false negatives: 4\n",
      "-------------------------------------------\n",
      "Average number of similar users: 87.2\n",
      "Average number of false positives: 79.2\n",
      "Average number of false negatives: 2.0\n",
      "===========================================\n",
      "100 Hash functions Results\n",
      "-------------------------------------------\n",
      "Number of similar users: 24\n",
      "Number of false positives: 17\n",
      "Number of false negatives: 3\n",
      "-------------------------------------------\n",
      "Number of similar users: 28\n",
      "Number of false positives: 18\n",
      "Number of false negatives: 0\n",
      "-------------------------------------------\n",
      "Number of similar users: 20\n",
      "Number of false positives: 12\n",
      "Number of false negatives: 2\n",
      "-------------------------------------------\n",
      "Number of similar users: 27\n",
      "Number of false positives: 20\n",
      "Number of false negatives: 3\n",
      "-------------------------------------------\n",
      "Number of similar users: 47\n",
      "Number of false positives: 38\n",
      "Number of false negatives: 1\n",
      "-------------------------------------------\n",
      "Average number of similar users: 29.2\n",
      "Average number of false positives: 21.0\n",
      "Average number of false negatives: 1.8\n",
      "===========================================\n",
      "200 Hash functions Results\n",
      "-------------------------------------------\n",
      "Number of similar users: 14\n",
      "Number of false positives: 5\n",
      "Number of false negatives: 1\n",
      "-------------------------------------------\n",
      "Number of similar users: 13\n",
      "Number of false positives: 6\n",
      "Number of false negatives: 3\n",
      "-------------------------------------------\n",
      "Number of similar users: 25\n",
      "Number of false positives: 16\n",
      "Number of false negatives: 1\n",
      "-------------------------------------------\n",
      "Number of similar users: 18\n",
      "Number of false positives: 8\n",
      "Number of false negatives: 0\n",
      "-------------------------------------------\n",
      "Number of similar users: 14\n",
      "Number of false positives: 5\n",
      "Number of false negatives: 1\n",
      "-------------------------------------------\n",
      "Average number of similar users: 16.8\n",
      "Average number of false positives: 8.0\n",
      "Average number of false negatives: 1.2\n",
      "===========================================\n",
      "Elapsed time: 273.678423166275 seconds\n"
     ]
    }
   ],
   "source": [
    "# Import library to generate random numbers\n",
    "import random\n",
    "\n",
    "# Calculate the start time of the code execution\n",
    "start_time = time.time()\n",
    "\n",
    "# Set the maximum value for the hash functions and the list of number of hash functions to test\n",
    "R = 10009\n",
    "num_functions_list = [50, 100, 200]\n",
    "\n",
    "# Set the similarity threshold and the number of runs\n",
    "threshold = 0.5\n",
    "n_runs = 5\n",
    "\n",
    "# Iterate over the list of number of hash functions to test\n",
    "for num_functions in num_functions_list:\n",
    "    # Create lists to store the results for each run\n",
    "    similar_users_lengths = []\n",
    "    avg_similar_users = []\n",
    "    avg_false_positives = 0\n",
    "    avg_false_negatives = 0\n",
    "\n",
    "    # Print the current number of hash functions being tested\n",
    "    print(f\"{num_functions} Hash functions Results\")\n",
    "    print(\"-------------------------------------------\")\n",
    "    \n",
    "    # Set the seed for reproducibility\n",
    "    random.seed(1000)\n",
    "    \n",
    "    # Perform multiple runs with different random hash functions\n",
    "    for n in range(n_runs):\n",
    "        # Create a list to store the different hash functions based on the randomly generated a and b values\n",
    "        hash_functions = []\n",
    "        for j in range(num_functions):\n",
    "            # Excluded 0 to avoid hash functions be simplified to a constant value of b.\n",
    "            # Upper range was set to R-1 to avoid the hash_value be 0 for multiple pairs. \n",
    "            # eg hash_value = (10009 * 1 + 10009) % 10009 = 0\n",
    "            a = random.randint(1, R-1)\n",
    "            b = random.randint(0, R-1)\n",
    "            hash_functions.append((a, b))\n",
    "        \n",
    "        # Create a dictionary to store the MinHash signatures for each user\n",
    "        minhash_signatures = {}\n",
    "        \n",
    "        # Generate all signatures for each user\n",
    "        for user_id, movies in movies_user.items():\n",
    "            # Set the initial values to positive infinity to ensure that \n",
    "            # the first hash value encountered for each movie will be smaller than the initial infinity value\n",
    "            signature = [float('inf')] * num_functions\n",
    "            \n",
    "            # Calculate hash values for each movie and store as signature the minimum one\n",
    "            for movie_id in movies:\n",
    "                for i, (a, b) in enumerate(hash_functions):\n",
    "                    hash_value = (a * movie_id + b) % R\n",
    "                    signature[i] = min(signature[i], hash_value)\n",
    "                    \n",
    "            # Store the MinHash signature for the user\n",
    "            minhash_signatures[user_id] = signature\n",
    "        \n",
    "        # Create a list to store the similar users and variables to count false positives and false negatives\n",
    "        similar_users = []\n",
    "        false_positives = 0\n",
    "        false_negatives = 0\n",
    "\n",
    "        # Calculate the similarities between all pairs of users\n",
    "        for user1 in minhash_signatures:\n",
    "            signature1 = minhash_signatures[user1]\n",
    "            for user2 in minhash_signatures:\n",
    "                # Avoid duplicate pairs by only considering pairs where user1 < user2\n",
    "                if user1 < user2:\n",
    "                    signature2 = minhash_signatures[user2]\n",
    "                    \n",
    "                    # Calculate the similarity between the two signatures using the get_similarity function\n",
    "                    similarity = get_similarity(signature1, signature2)\n",
    "                    \n",
    "                    # If the similarity is greater than or equal to the threshold,\n",
    "                    # add the pair of users to the list of similar users\n",
    "                    if similarity >= threshold:\n",
    "                        similar_users.append((user1, user2))\n",
    "                        \n",
    "                        # Check against exact Jaccard similarity for false positives/negatives\n",
    "                        jaccard_similarity = jaccard_sim(movies_user[user1], movies_user[user2])\n",
    "                        if jaccard_similarity < threshold:\n",
    "                            false_positives += 1\n",
    "                    else:\n",
    "                        # Check against exact Jaccard similarity for false positives/negatives\n",
    "                        jaccard_similarity = jaccard_sim(movies_user[user1], movies_user[user2])\n",
    "                        if jaccard_similarity >= threshold:\n",
    "                            false_negatives += 1\n",
    "        \n",
    "        # Update the lists and variables with the results from this run\n",
    "        similar_users_lengths.append(len(similar_users))\n",
    "        avg_similar_users.append(similar_users)\n",
    "        avg_false_positives += false_positives\n",
    "        avg_false_negatives += false_negatives\n",
    "\n",
    "        # Print the results for this run\n",
    "        print(\"Number of similar users:\", len(similar_users))\n",
    "        print(\"Number of false positives:\", false_positives)\n",
    "        print(\"Number of false negatives:\", false_negatives)\n",
    "        print(\"-------------------------------------------\")\n",
    "\n",
    "    # Calculate the average number of false positives and false negatives over all runs\n",
    "    avg_false_positives /= n_runs\n",
    "    avg_false_negatives /= n_runs\n",
    "\n",
    "    # Print the average results over all runs\n",
    "    print(\"Average number of similar users:\", sum(similar_users_lengths) / len(similar_users_lengths))\n",
    "    print(\"Average number of false positives:\", avg_false_positives)\n",
    "    print(\"Average number of false negatives:\", avg_false_negatives)\n",
    "    print(\"===========================================\")\n",
    "\n",
    "# Calculate the end time and elapsed time of the code execution\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# Print the elapsed time of the code execution\n",
    "print(f\"Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725c1436",
   "metadata": {},
   "source": [
    "#### Comments\n",
    "\n",
    "Generally, the number of hash functions used in the MinHash algorithm can affect the number of false positives and false negatives returned by the **`get_similar_users()`** function. \n",
    "\n",
    "Increasing the number of hash functions can improve the accuracy of the MinHash algorithm in estimating the Jaccard similarity between pairs of users. This can result in a lower number of false positives and false negatives. However, using more hash functions also increases the computational cost of the algorithm and may result in slower performance.\n",
    "\n",
    "On the other hand, decreasing the number of hash functions can reduce the computational cost of the MinHash algorithm and make it faster. However, using fewer hash functions can also reduce the accuracy of the algorithm in estimating the Jaccard similarity between pairs of users. This can result in a higher number of false positives and false negatives.\n",
    "\n",
    "In this project, and since the exact Jaccard similarity was already calculated, we validated the similarities from the MinHash algorithm ouput by comparing them versus the \"ground truth\". As it can be seen from the results, in every run and regardeless of the number of functions, the actual number of similar users was always 10 after deducting the number of false positives and by adding the number of false negatives (10 was also the actual number of similar users based on the the Jaccard similarity)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b79712c",
   "metadata": {},
   "source": [
    "### <hr>`Task 4 - Locate similar users using LSH index`\n",
    "\n",
    "Using a set of 200 hash functions break up the signatures into **b bands with r hash functions per band** (br=200) and implement **Locality Sensitive Hashing**.\n",
    "Recall that with LSH we **first** locate users that are similar (have the same mini-signatures) across **at least one** band and **then** assess their true similarity using their **initial** representations. Use the following two instances of LSH:\n",
    "- LSH instance 1: b = 25, r = 8\n",
    "- LSH instance 2: b = 40, r = 5\n",
    "Using each instance find the pair of users with similarity **at least 0.5** and report:\n",
    "- The number of true pairs returned (true positives).\n",
    "- The number of similarity evaluations performed using the initial representations.\n",
    "Report the averages for **5 different runs** using different functions.\n",
    "Based on the reported results, what do we **gain/loose** by using LSH instead of directly comparing users on their true representations?<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a33c5e",
   "metadata": {},
   "source": [
    "#### The Process of Locating Similar Users Using the LSH Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c0454a",
   "metadata": {},
   "source": [
    "To begin with, we set the number of bands (**b**) and number of hash functions per band (**r**) for each instance of LSH. Specifically, we assigned 25 bands and 8 hash functions for the first instance (**b1 = 25**, **r1 = 8**), and 40 bands and 5 hash functions for the second instance (**b2 = 40**, **r2 = 5**).\n",
    "\n",
    "Then, we created two dictionaries, **bands1** and **bands2**, to store the bands for each user.\n",
    "\n",
    "Next, we iterated over the dataset and created the bands for each user. For each user, we split their minhash signature into bands based on the specified number of hash functions for each LSH instance. These bands were then assigned to the corresponding user in the bands1 and bands2 dictionaries.\n",
    "\n",
    "Moving forward, we aimed to find similar users using each instance of LSH. To achieve this, we initialized two lists, **true_pairs1** and **true_pairs2**, to store the pairs of users that are identified as similar based on their LSH bands.\n",
    "\n",
    "We employed nested loops to compare each pair of users, ensuring that each pair was considered only once. For each pair, we checked if they had at least one band in common by iterating through their bands. If a common band was found, we proceeded to calculate their Jaccard similarity using their initial movie representations.\n",
    "\n",
    "During the similarity calculation, we kept track of the number of similarity evaluations performed. For each LSH instance, we incremented the respective **evaluations1** or **evaluations2** counter.\n",
    "\n",
    "If the Jaccard similarity between a pair of users exceeded a specified threshold, we considered them a true pair and appended them to the corresponding true pairs list (**true_pairs1** or **true_pairs2**).\n",
    "\n",
    "Finally, we printed the results for each instance of LSH, including the number of true pairs found and the number of similarity evaluations performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf63ff42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSH instance 1: True pairs: 2, Similarity evaluations: 38\n",
      "LSH instance 2: True pairs: 10, Similarity evaluations: 1691\n",
      "-------------------------------------------------------------------------------------\n",
      "LSH instance 1: True pairs: 2, Similarity evaluations: 35\n",
      "LSH instance 2: True pairs: 8, Similarity evaluations: 2050\n",
      "-------------------------------------------------------------------------------------\n",
      "LSH instance 1: True pairs: 3, Similarity evaluations: 48\n",
      "LSH instance 2: True pairs: 7, Similarity evaluations: 2447\n",
      "-------------------------------------------------------------------------------------\n",
      "LSH instance 1: True pairs: 3, Similarity evaluations: 53\n",
      "LSH instance 2: True pairs: 8, Similarity evaluations: 2043\n",
      "-------------------------------------------------------------------------------------\n",
      "LSH instance 1: True pairs: 2, Similarity evaluations: 38\n",
      "LSH instance 2: True pairs: 6, Similarity evaluations: 1859\n",
      "-------------------------------------------------------------------------------------\n",
      "===========================================================================================\n",
      "LSH instance 1: Average true pairs: 2.4, Average similarity evaluations: 42.4\n",
      "LSH instance 2: Average true pairs: 7.8, Average similarity evaluations: 2018.0\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 85.89039373397827 seconds\n"
     ]
    }
   ],
   "source": [
    "# Calculate the start time of the code execution\n",
    "start_time = time.time()\n",
    "\n",
    "import random\n",
    "\n",
    "# Set the number of bands and hash functions for each instance of LSH\n",
    "b1 = 25\n",
    "r1 = 8\n",
    "\n",
    "b2 = 40\n",
    "r2 = 5\n",
    "\n",
    "# Set the number of runs and the number of hash functions to use\n",
    "n_runs = 5\n",
    "num_functions = 200\n",
    "\n",
    "# Set the maximum value for the hash functions\n",
    "R = 10009\n",
    "\n",
    "# Create lists to store the results of each run\n",
    "results1 = []\n",
    "results2 = []\n",
    "\n",
    " \n",
    "# Set the seed for reproducibility\n",
    "random.seed(1000)\n",
    "\n",
    "# Perform multiple runs with different random hash functions\n",
    "for n in range(n_runs):\n",
    "    # Create a list to store the different hash functions based on the randomly generated a and b values\n",
    "    hash_functions = []\n",
    "    for j in range(num_functions):\n",
    "        # Excluded 0 to avoid hash functions be simplified to a constant value of b.\n",
    "        # Upper range was set to R-1 to avoid the hash_value be 0 for multiple pairs. \n",
    "        # eg hash_value = (10009 * 1 + 10009) % 10009 = 0\n",
    "        a = random.randint(1, R-1)\n",
    "        b = random.randint(0, R-1)\n",
    "        hash_functions.append((a, b))\n",
    "    \n",
    "    # Create a dictionary to store the MinHash signatures for each user\n",
    "    minhash_signatures = {}\n",
    "    \n",
    "    # Generate all signatures for each user\n",
    "    for user_id, movies in movies_user.items():\n",
    "        # Set the initial values to positive infinity to ensure that \n",
    "        # the first hash value encountered for each movie will be smaller than the initial infinity value\n",
    "        signature = [float('inf')] * num_functions\n",
    "        \n",
    "        # Calculate hash values for each movie and store as signature the minimum one\n",
    "        for movie_id in movies:\n",
    "            for i, (a, b) in enumerate(hash_functions):\n",
    "                hash_value = (a * movie_id + b) % R\n",
    "                signature[i] = min(signature[i], hash_value)\n",
    "                \n",
    "        # Store the MinHash signature for the user\n",
    "        minhash_signatures[user_id] = signature\n",
    "\n",
    "    # Create dictionaries to store the bands for each user\n",
    "    bands1 = {}\n",
    "    bands2 = {}\n",
    "\n",
    "    # Iterate over the dataset and create the bands for each user using each set of minhash signatures\n",
    "    for user_id, signature in minhash_signatures.items():\n",
    "        # Split the signature into bands for each instance of LSH\n",
    "        bands1[user_id] = [signature[i*r1:(i+1)*r1] for i in range(b1)]\n",
    "        bands2[user_id] = [signature[i*r2:(i+1)*r2] for i in range(b2)]\n",
    "\n",
    "    # Find similar users using each instance of LSH\n",
    "    true_pairs1 = []\n",
    "    evaluations1 = 0\n",
    "\n",
    "    true_pairs2 = []\n",
    "    evaluations2 = 0\n",
    "\n",
    "    for user1, bands in bands1.items():\n",
    "        for user2, other_bands in bands1.items():\n",
    "            if user1 < user2:\n",
    "                # Check if the users have at least one band in common\n",
    "                common_band = False\n",
    "                for band, other_band in zip(bands, other_bands):\n",
    "                    if band == other_band:\n",
    "                        common_band = True\n",
    "                        break\n",
    "                \n",
    "                # If the users have at least one band in common,\n",
    "                # calculate their Jaccard similarity using their initial representations\n",
    "                if common_band:\n",
    "                    evaluations1 += 1\n",
    "                    \n",
    "                    jaccard_similarity = jaccard_sim(movies_user[user1], movies_user[user2])\n",
    "                    if jaccard_similarity >= threshold:\n",
    "                        true_pairs1.append((user1, user2))\n",
    "\n",
    "    for user1, bands in bands2.items():\n",
    "        for user2, other_bands in bands2.items():\n",
    "            if user1 < user2:\n",
    "                # Check if the users have at least one band in common\n",
    "                common_band = False\n",
    "                for band, other_band in zip(bands, other_bands):\n",
    "                    if band == other_band:\n",
    "                        common_band = True\n",
    "                        break\n",
    "                \n",
    "                # If the users have at least one band in common,\n",
    "                # calculate their Jaccard similarity using their initial representations\n",
    "                if common_band:\n",
    "                    evaluations2 += 1\n",
    "                    \n",
    "                    jaccard_similarity = jaccard_sim(movies_user[user1], movies_user[user2])\n",
    "                    if jaccard_similarity >= threshold:\n",
    "                        true_pairs2.append((user1, user2))\n",
    "\n",
    "    # Store the results of this run\n",
    "    results1.append((len(true_pairs1), evaluations1))\n",
    "    results2.append((len(true_pairs2), evaluations2))\n",
    "    \n",
    "    # Print the average results for each instance of LSH\n",
    "    print(f\"LSH instance 1: True pairs: {len(true_pairs1)}, Similarity evaluations: {evaluations1}\")\n",
    "    print(f\"LSH instance 2: True pairs: {len(true_pairs2)}, Similarity evaluations: {evaluations2}\")\n",
    "    print(\"-------------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "# Calculate the average number of true pairs and similarity evaluations for each instance of LSH\n",
    "avg_true_pairs1 = sum([x[0] for x in results1]) / len(results1)\n",
    "avg_evaluations1 = sum([x[1] for x in results1]) / len(results1)\n",
    "\n",
    "avg_true_pairs2 = sum([x[0] for x in results2]) / len(results2)\n",
    "avg_evaluations2 = sum([x[1] for x in results2]) / len(results2)\n",
    "\n",
    "# Print the average results for each instance of LSH\n",
    "print(\"===========================================================================================\")\n",
    "print(f\"LSH instance 1: Average true pairs: {avg_true_pairs1}, Average similarity evaluations: {avg_evaluations1}\")\n",
    "print(f\"LSH instance 2: Average true pairs: {avg_true_pairs2}, Average similarity evaluations: {avg_evaluations2}\")\n",
    "\n",
    "# Calculate the end time and elapsed time of the code execution\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(\"-------------------------------------------------------------------------------------\")\n",
    "# Print the elapsed time of the code execution\n",
    "print(f\"Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bf724f",
   "metadata": {},
   "source": [
    "#### Comments\n",
    "The code runs LSH with two different instances: b=25, r=8 and b=40, r=5. For each instance, it computes the average number of true positives and the average number of similarity evaluations over 5 different runs.\n",
    "\n",
    "By using LSH instead of directly comparing users on their true representations, we can reduce the number of similarity evaluations that need to be performed. This can make the algorithm faster for large datasets. However, LSH is an approximate algorithm and may not find all pairs of similar users. This can result in a lower number of true positives compared to directly comparing users on their true representations.\n",
    "\n",
    "In this project, it is noticeable that the elapsed computational time has been decreased due to the reduced number of similarity evaluations, but as it was expected the number of true pairs was decreased as well. The \"ground truth\" was 10 pairs but in both LSH instances we found less than 10 pairs (2.4 and 7.8 respectivelly). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44ae649",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
